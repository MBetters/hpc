add mpi-openacc codes into this repository

-- Eric ----------------------------------------------------------------
So far we have MPI EP working, currently working towards OpenACC
A lot of learning about OpenACC to do still

Haven't gotten MPI+OpenACC to compile yet, will worry about that after
OpenACC code is done

in the EP-MPI directory you can just do
    make EP CLASS=A
to compile and then
    mpiexec -n 4 ./bin/ep.A.x
to run it, the program should take about 35 seconds with 1 processor
and about 1/4 that with 4 processors

OpenACC is still being worked on
We were having problems with how much stuff is going on
in the NAS NPB Makefiles, so we reorganized all essential files and made
our own very simple Makefile for now...
    make test
will compile it, but right now it's not done yet, so not much will happen
except for compiler error messages
all of the OpenACC code
-- Aamir ----------------------------------------------------------------
Current status:
MPI+OpenACC code compiles and links correctly for EP! Sample code is in the mpi-openacc/samples/openacc/hello directory. Here are the GPU numbers for reference:
1080 GPU ~ -ta=tesla:cc60
Tesla K80 ~ -ta=tesla:cc35
Things we plan to do for next week:
- pgprof
- mess with block size
- GNUPlot based on block size
- See why the teslas are slow
- Write a report
- Manager-worker/queue setup
- How we split the workload
- Block size and performance
- Scalability 

Notes for 4/18/17:
Have to implement different GPU performance models
(static allocation)
- OpenACC only on 1080's
- OpenACC only on Teslas
- OpenACC + MPI on only Teslas
workload, device, distribution (pattern)
- tables (GNUPlot, no GNUPlot)
- poster ~ 3-column poster
1st column - EP Benchmark (pictures, minimum text)
2nd column - programming model/design
3rd column - performance/numbers and statistics
MPI+OpenACC vs. OpenACC vs. sequential
GNUPlots/3dplot/heapplot
threads per block allocation?
pgprof, nvidia-smi
------------------------------------------------------------------------
